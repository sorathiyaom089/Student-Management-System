#!/usr/bin/env python3
"""
Performance Metrics Matrix Visualization
========================================
Creates a comprehensive performance metrics heatmap showing precision, recall, and f1-score
for different fault types, similar to the reference image provided.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import classification_report, precision_recall_fscore_support
import warnings
warnings.filterwarnings('ignore')

def create_performance_metrics_matrix():
    """Create a beautiful performance metrics matrix visualization"""
    
    # Sample data based on our engine fault detection results
    # In real scenario, this would come from classification_report
    fault_types = ['Normal', 'Overheating', 'Low Pressure', 'High Vibration']
    metrics = ['precision', 'recall', 'f1-score']
    
    # Performance data (realistic values based on our model results)
    performance_data = np.array([
        [1.00, 1.00, 1.00],  # Normal: Perfect performance
        [1.00, 1.00, 1.00],  # Overheating: Perfect performance  
        [1.00, 1.00, 1.00],  # Low Pressure: Perfect performance
        [1.00, 0.97, 0.98],  # High Vibration: Slight challenge (realistic)
    ])
    
    # Add overall metrics
    accuracy_row = np.array([[1.00, 1.00, 1.00]])  # Overall accuracy
    macro_avg_row = np.array([[1.00, 0.99, 1.00]]) # Macro average
    weighted_avg_row = np.array([[1.00, 1.00, 1.00]]) # Weighted average
    
    # Combine all data
    all_data = np.vstack([performance_data, accuracy_row, macro_avg_row, weighted_avg_row])
    
    # Create labels
    all_labels = fault_types + ['accuracy', 'macro avg', 'weighted avg']
    
    # Create DataFrame for easier handling
    df = pd.DataFrame(all_data, index=all_labels, columns=metrics)
    
    # Create the heatmap
    plt.figure(figsize=(10, 8))
    
    # Use a colormap similar to your reference image (yellow-green-blue)
    colors = ['#440154', '#31688e', '#35b779', '#fde725']  # Purple to yellow
    custom_cmap = sns.blend_palette(colors, as_cmap=True)
    
    # Create the heatmap
    ax = sns.heatmap(df, 
                     annot=True,           # Show values
                     fmt='.2f',            # Format to 2 decimal places
                     cmap=custom_cmap,     # Custom colormap
                     cbar_kws={'label': ''}, # Colorbar label
                     square=True,          # Square cells
                     linewidths=0.5,       # Cell borders
                     linecolor='white',    # Border color
                     annot_kws={'size': 12, 'weight': 'bold'}, # Text formatting
                     vmin=0.96,            # Minimum value for color scale
                     vmax=1.00)            # Maximum value for color scale
    
    # Customize the plot
    plt.title('Performance Metrics Matrix', 
              fontsize=16, fontweight='bold', pad=20)
    
    # Rotate labels for better readability
    plt.xticks(rotation=0, ha='center')
    plt.yticks(rotation=0, ha='right')
    
    # Adjust layout
    plt.tight_layout()
    
    # Save the plot
    output_file = 'performance_metrics_matrix.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    
    print("ðŸŽ¨ Performance Metrics Matrix Created!")
    print(f"ðŸ“Š Saved as: {output_file}")
    
    # Show the plot
    plt.show()
    
    return df

def create_detailed_classification_report():
    """Create a more detailed version with actual classification metrics"""
    
    # Simulated classification results (based on our engine fault detection)
    y_true = np.array([0]*800 + [1]*550 + [2]*750 + [3]*700)  # True labels
    y_pred = np.array([0]*800 + [1]*550 + [2]*730 + [3]*720)  # Predicted labels
    
    # Add some realistic errors
    # Make some predictions wrong for more realistic results
    np.random.seed(42)
    error_indices = np.random.choice(len(y_pred), size=150, replace=False)
    for idx in error_indices:
        if y_true[idx] < 3:
            y_pred[idx] = y_true[idx] + 1
        else:
            y_pred[idx] = y_true[idx] - 1
    
    # Generate classification report
    target_names = ['Normal', 'Overheating', 'Low Pressure', 'High Vibration']
    
    # Get detailed metrics
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, target_names=target_names, average=None
    )
    
    # Calculate macro and weighted averages
    macro_precision = np.mean(precision)
    macro_recall = np.mean(recall) 
    macro_f1 = np.mean(f1)
    
    weighted_precision = np.average(precision, weights=support)
    weighted_recall = np.average(recall, weights=support)
    weighted_f1 = np.average(f1, weights=support)
    
    accuracy = np.mean(y_true == y_pred)
    
    # Create comprehensive data matrix
    metrics_data = np.array([
        [precision[0], recall[0], f1[0]],  # Normal
        [precision[1], recall[1], f1[1]],  # Overheating
        [precision[2], recall[2], f1[2]],  # Low Pressure
        [precision[3], recall[3], f1[3]],  # High Vibration
        [accuracy, accuracy, accuracy],     # Accuracy
        [macro_precision, macro_recall, macro_f1],  # Macro avg
        [weighted_precision, weighted_recall, weighted_f1]  # Weighted avg
    ])
    
    return metrics_data, target_names + ['accuracy', 'macro avg', 'weighted avg']

def main():
    """Main execution function"""
    
    print("ðŸŽ¯ PERFORMANCE METRICS MATRIX GENERATOR")
    print("=" * 50)
    
    # Option 1: Simple demonstration matrix (like your reference)
    print("\nðŸ“Š Creating Performance Metrics Matrix...")
    df = create_performance_metrics_matrix()
    
    print("\nâœ… Matrix Values:")
    print(df.round(3))
    
    # Option 2: More realistic version
    print("\nðŸ” Want to create a version with realistic classification data? (y/n): ", end="")
    
    # For automatic execution, let's create both
    print("y")
    print("\nðŸ“ˆ Creating Detailed Classification Report Matrix...")
    
    detailed_data, labels = create_detailed_classification_report()
    
    # Create detailed visualization
    plt.figure(figsize=(10, 8))
    
    df_detailed = pd.DataFrame(detailed_data, 
                              index=labels, 
                              columns=['precision', 'recall', 'f1-score'])
    
    # Create heatmap with same style as reference
    ax = sns.heatmap(df_detailed, 
                     annot=True, 
                     fmt='.2f',
                     cmap='viridis',  # Yellow-green colormap like reference
                     square=True,
                     linewidths=0.5,
                     linecolor='white',
                     annot_kws={'size': 12, 'weight': 'bold'},
                     cbar_kws={'shrink': 0.8})
    
    plt.title('Performance Metrics Matrix - Detailed Classification', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xticks(rotation=0, ha='center')
    plt.yticks(rotation=0, ha='right')
    plt.tight_layout()
    
    # Save detailed version
    detailed_file = 'performance_metrics_matrix_detailed.png'
    plt.savefig(detailed_file, dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    
    print(f"ðŸ“Š Detailed version saved as: {detailed_file}")
    plt.show()
    
    print("\nðŸŽ‰ Performance Metrics Matrix Generation Complete!")
    print("\nðŸ“ Files Created:")
    print("  ðŸ“Š performance_metrics_matrix.png")
    print("  ðŸ“Š performance_metrics_matrix_detailed.png")

if __name__ == "__main__":
    main()